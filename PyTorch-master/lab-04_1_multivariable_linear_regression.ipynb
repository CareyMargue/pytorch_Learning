{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4-1: Multivariate Linear Regression 多个特征值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Seungjae Lee (이승재)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    We use elemental PyTorch to implement linear regression here. However, in most actual applications, abstractions such as <code>nn.Module</code> or <code>nn.Linear</code> are used.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ H(x_1, x_2, x_3) = x_1w_1 + x_2w_2 + x_3w_3 + b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ cost(W, b) = \\frac{1}{m} \\sum^m_{i=1} \\left( H(x^{(i)}) - y^{(i)} \\right)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - $H(x)$: 주어진 $x$ 값에 대해 예측을 어떻게 할 것인가\n",
    " - $cost(W, b)$: $H(x)$ 가 $y$ 를 얼마나 잘 예측했는가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a193ff3570>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For reproducibility\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Data Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use fake data for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])\n",
    "x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])\n",
    "x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/10000 w1: 0.294 w2: 0.297 w3: 0.297 b: 0.003 Cost: 29661.800781\n",
      "Epoch  100/10000 w1: 0.674 w2: 0.676 w3: 0.676 b: 0.008 Cost: 1.563634\n",
      "Epoch  200/10000 w1: 0.679 w2: 0.677 w3: 0.677 b: 0.008 Cost: 1.497607\n",
      "Epoch  300/10000 w1: 0.684 w2: 0.677 w3: 0.677 b: 0.008 Cost: 1.435026\n",
      "Epoch  400/10000 w1: 0.689 w2: 0.678 w3: 0.678 b: 0.008 Cost: 1.375730\n",
      "Epoch  500/10000 w1: 0.694 w2: 0.678 w3: 0.678 b: 0.009 Cost: 1.319511\n",
      "Epoch  600/10000 w1: 0.699 w2: 0.679 w3: 0.679 b: 0.009 Cost: 1.266222\n",
      "Epoch  700/10000 w1: 0.704 w2: 0.679 w3: 0.679 b: 0.009 Cost: 1.215696\n",
      "Epoch  800/10000 w1: 0.709 w2: 0.679 w3: 0.679 b: 0.009 Cost: 1.167818\n",
      "Epoch  900/10000 w1: 0.713 w2: 0.680 w3: 0.680 b: 0.009 Cost: 1.122429\n",
      "Epoch 1000/10000 w1: 0.718 w2: 0.680 w3: 0.680 b: 0.009 Cost: 1.079378\n",
      "Epoch 1100/10000 w1: 0.722 w2: 0.680 w3: 0.680 b: 0.009 Cost: 1.038584\n",
      "Epoch 1200/10000 w1: 0.727 w2: 0.681 w3: 0.681 b: 0.010 Cost: 0.999894\n",
      "Epoch 1300/10000 w1: 0.731 w2: 0.681 w3: 0.681 b: 0.010 Cost: 0.963217\n",
      "Epoch 1400/10000 w1: 0.735 w2: 0.681 w3: 0.681 b: 0.010 Cost: 0.928421\n",
      "Epoch 1500/10000 w1: 0.739 w2: 0.681 w3: 0.681 b: 0.010 Cost: 0.895453\n",
      "Epoch 1600/10000 w1: 0.743 w2: 0.682 w3: 0.682 b: 0.010 Cost: 0.864161\n",
      "Epoch 1700/10000 w1: 0.746 w2: 0.682 w3: 0.682 b: 0.010 Cost: 0.834503\n",
      "Epoch 1800/10000 w1: 0.750 w2: 0.682 w3: 0.682 b: 0.010 Cost: 0.806375\n",
      "Epoch 1900/10000 w1: 0.754 w2: 0.682 w3: 0.682 b: 0.010 Cost: 0.779696\n",
      "Epoch 2000/10000 w1: 0.757 w2: 0.682 w3: 0.682 b: 0.011 Cost: 0.754389\n",
      "Epoch 2100/10000 w1: 0.760 w2: 0.682 w3: 0.682 b: 0.011 Cost: 0.730373\n",
      "Epoch 2200/10000 w1: 0.764 w2: 0.682 w3: 0.682 b: 0.011 Cost: 0.707607\n",
      "Epoch 2300/10000 w1: 0.767 w2: 0.682 w3: 0.682 b: 0.011 Cost: 0.685989\n",
      "Epoch 2400/10000 w1: 0.770 w2: 0.682 w3: 0.682 b: 0.011 Cost: 0.665497\n",
      "Epoch 2500/10000 w1: 0.773 w2: 0.682 w3: 0.682 b: 0.011 Cost: 0.646035\n",
      "Epoch 2600/10000 w1: 0.776 w2: 0.682 w3: 0.682 b: 0.011 Cost: 0.627585\n",
      "Epoch 2700/10000 w1: 0.779 w2: 0.682 w3: 0.682 b: 0.012 Cost: 0.610050\n",
      "Epoch 2800/10000 w1: 0.782 w2: 0.682 w3: 0.682 b: 0.012 Cost: 0.593426\n",
      "Epoch 2900/10000 w1: 0.785 w2: 0.682 w3: 0.682 b: 0.012 Cost: 0.577643\n",
      "Epoch 3000/10000 w1: 0.788 w2: 0.682 w3: 0.682 b: 0.012 Cost: 0.562648\n",
      "Epoch 3100/10000 w1: 0.791 w2: 0.682 w3: 0.682 b: 0.012 Cost: 0.548429\n",
      "Epoch 3200/10000 w1: 0.793 w2: 0.682 w3: 0.682 b: 0.012 Cost: 0.534923\n",
      "Epoch 3300/10000 w1: 0.796 w2: 0.682 w3: 0.682 b: 0.012 Cost: 0.522093\n",
      "Epoch 3400/10000 w1: 0.798 w2: 0.682 w3: 0.682 b: 0.012 Cost: 0.509906\n",
      "Epoch 3500/10000 w1: 0.801 w2: 0.682 w3: 0.682 b: 0.012 Cost: 0.498317\n",
      "Epoch 3600/10000 w1: 0.803 w2: 0.681 w3: 0.681 b: 0.013 Cost: 0.487341\n",
      "Epoch 3700/10000 w1: 0.806 w2: 0.681 w3: 0.681 b: 0.013 Cost: 0.476888\n",
      "Epoch 3800/10000 w1: 0.808 w2: 0.681 w3: 0.681 b: 0.013 Cost: 0.466961\n",
      "Epoch 3900/10000 w1: 0.810 w2: 0.681 w3: 0.681 b: 0.013 Cost: 0.457528\n",
      "Epoch 4000/10000 w1: 0.812 w2: 0.681 w3: 0.681 b: 0.013 Cost: 0.448554\n",
      "Epoch 4100/10000 w1: 0.814 w2: 0.681 w3: 0.681 b: 0.013 Cost: 0.440042\n",
      "Epoch 4200/10000 w1: 0.817 w2: 0.680 w3: 0.680 b: 0.013 Cost: 0.431924\n",
      "Epoch 4300/10000 w1: 0.819 w2: 0.680 w3: 0.680 b: 0.013 Cost: 0.424216\n",
      "Epoch 4400/10000 w1: 0.821 w2: 0.680 w3: 0.680 b: 0.014 Cost: 0.416875\n",
      "Epoch 4500/10000 w1: 0.823 w2: 0.680 w3: 0.680 b: 0.014 Cost: 0.409898\n",
      "Epoch 4600/10000 w1: 0.825 w2: 0.679 w3: 0.679 b: 0.014 Cost: 0.403260\n",
      "Epoch 4700/10000 w1: 0.826 w2: 0.679 w3: 0.679 b: 0.014 Cost: 0.396934\n",
      "Epoch 4800/10000 w1: 0.828 w2: 0.679 w3: 0.679 b: 0.014 Cost: 0.390917\n",
      "Epoch 4900/10000 w1: 0.830 w2: 0.679 w3: 0.679 b: 0.014 Cost: 0.385192\n",
      "Epoch 5000/10000 w1: 0.832 w2: 0.678 w3: 0.678 b: 0.014 Cost: 0.379734\n",
      "Epoch 5100/10000 w1: 0.834 w2: 0.678 w3: 0.678 b: 0.014 Cost: 0.374534\n",
      "Epoch 5200/10000 w1: 0.835 w2: 0.678 w3: 0.678 b: 0.014 Cost: 0.369584\n",
      "Epoch 5300/10000 w1: 0.837 w2: 0.677 w3: 0.677 b: 0.015 Cost: 0.364857\n",
      "Epoch 5400/10000 w1: 0.839 w2: 0.677 w3: 0.677 b: 0.015 Cost: 0.360361\n",
      "Epoch 5500/10000 w1: 0.840 w2: 0.677 w3: 0.677 b: 0.015 Cost: 0.356061\n",
      "Epoch 5600/10000 w1: 0.842 w2: 0.676 w3: 0.676 b: 0.015 Cost: 0.351966\n",
      "Epoch 5700/10000 w1: 0.843 w2: 0.676 w3: 0.676 b: 0.015 Cost: 0.348054\n",
      "Epoch 5800/10000 w1: 0.845 w2: 0.676 w3: 0.676 b: 0.015 Cost: 0.344332\n",
      "Epoch 5900/10000 w1: 0.846 w2: 0.675 w3: 0.675 b: 0.015 Cost: 0.340761\n",
      "Epoch 6000/10000 w1: 0.848 w2: 0.675 w3: 0.675 b: 0.015 Cost: 0.337359\n",
      "Epoch 6100/10000 w1: 0.849 w2: 0.675 w3: 0.675 b: 0.016 Cost: 0.334109\n",
      "Epoch 6200/10000 w1: 0.851 w2: 0.674 w3: 0.674 b: 0.016 Cost: 0.330997\n",
      "Epoch 6300/10000 w1: 0.852 w2: 0.674 w3: 0.674 b: 0.016 Cost: 0.328017\n",
      "Epoch 6400/10000 w1: 0.853 w2: 0.674 w3: 0.674 b: 0.016 Cost: 0.325185\n",
      "Epoch 6500/10000 w1: 0.854 w2: 0.673 w3: 0.673 b: 0.016 Cost: 0.322460\n",
      "Epoch 6600/10000 w1: 0.856 w2: 0.673 w3: 0.673 b: 0.016 Cost: 0.319850\n",
      "Epoch 6700/10000 w1: 0.857 w2: 0.673 w3: 0.673 b: 0.016 Cost: 0.317359\n",
      "Epoch 6800/10000 w1: 0.858 w2: 0.672 w3: 0.672 b: 0.016 Cost: 0.314968\n",
      "Epoch 6900/10000 w1: 0.859 w2: 0.672 w3: 0.672 b: 0.016 Cost: 0.312671\n",
      "Epoch 7000/10000 w1: 0.861 w2: 0.671 w3: 0.671 b: 0.016 Cost: 0.310472\n",
      "Epoch 7100/10000 w1: 0.862 w2: 0.671 w3: 0.671 b: 0.017 Cost: 0.308366\n",
      "Epoch 7200/10000 w1: 0.863 w2: 0.671 w3: 0.671 b: 0.017 Cost: 0.306333\n",
      "Epoch 7300/10000 w1: 0.864 w2: 0.670 w3: 0.670 b: 0.017 Cost: 0.304398\n",
      "Epoch 7400/10000 w1: 0.865 w2: 0.670 w3: 0.670 b: 0.017 Cost: 0.302529\n",
      "Epoch 7500/10000 w1: 0.866 w2: 0.669 w3: 0.669 b: 0.017 Cost: 0.300733\n",
      "Epoch 7600/10000 w1: 0.867 w2: 0.669 w3: 0.669 b: 0.017 Cost: 0.299001\n",
      "Epoch 7700/10000 w1: 0.868 w2: 0.668 w3: 0.668 b: 0.017 Cost: 0.297342\n",
      "Epoch 7800/10000 w1: 0.869 w2: 0.668 w3: 0.668 b: 0.017 Cost: 0.295737\n",
      "Epoch 7900/10000 w1: 0.870 w2: 0.668 w3: 0.668 b: 0.017 Cost: 0.294198\n",
      "Epoch 8000/10000 w1: 0.871 w2: 0.667 w3: 0.667 b: 0.018 Cost: 0.292709\n",
      "Epoch 8100/10000 w1: 0.872 w2: 0.667 w3: 0.667 b: 0.018 Cost: 0.291277\n",
      "Epoch 8200/10000 w1: 0.873 w2: 0.666 w3: 0.666 b: 0.018 Cost: 0.289892\n",
      "Epoch 8300/10000 w1: 0.874 w2: 0.666 w3: 0.666 b: 0.018 Cost: 0.288554\n",
      "Epoch 8400/10000 w1: 0.875 w2: 0.665 w3: 0.665 b: 0.018 Cost: 0.287267\n",
      "Epoch 8500/10000 w1: 0.876 w2: 0.665 w3: 0.665 b: 0.018 Cost: 0.286015\n",
      "Epoch 8600/10000 w1: 0.877 w2: 0.665 w3: 0.665 b: 0.018 Cost: 0.284808\n",
      "Epoch 8700/10000 w1: 0.878 w2: 0.664 w3: 0.664 b: 0.018 Cost: 0.283635\n",
      "Epoch 8800/10000 w1: 0.879 w2: 0.664 w3: 0.664 b: 0.018 Cost: 0.282510\n",
      "Epoch 8900/10000 w1: 0.880 w2: 0.663 w3: 0.663 b: 0.018 Cost: 0.281409\n",
      "Epoch 9000/10000 w1: 0.880 w2: 0.663 w3: 0.663 b: 0.019 Cost: 0.280348\n",
      "Epoch 9100/10000 w1: 0.881 w2: 0.662 w3: 0.662 b: 0.019 Cost: 0.279307\n",
      "Epoch 9200/10000 w1: 0.882 w2: 0.662 w3: 0.662 b: 0.019 Cost: 0.278314\n",
      "Epoch 9300/10000 w1: 0.883 w2: 0.661 w3: 0.661 b: 0.019 Cost: 0.277338\n",
      "Epoch 9400/10000 w1: 0.884 w2: 0.661 w3: 0.661 b: 0.019 Cost: 0.276389\n",
      "Epoch 9500/10000 w1: 0.884 w2: 0.661 w3: 0.661 b: 0.019 Cost: 0.275472\n",
      "Epoch 9600/10000 w1: 0.885 w2: 0.660 w3: 0.660 b: 0.019 Cost: 0.274581\n",
      "Epoch 9700/10000 w1: 0.886 w2: 0.660 w3: 0.660 b: 0.019 Cost: 0.273710\n",
      "Epoch 9800/10000 w1: 0.887 w2: 0.659 w3: 0.659 b: 0.019 Cost: 0.272862\n",
      "Epoch 9900/10000 w1: 0.887 w2: 0.659 w3: 0.659 b: 0.019 Cost: 0.272027\n",
      "Epoch 10000/10000 w1: 0.888 w2: 0.658 w3: 0.658 b: 0.020 Cost: 0.271221\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "w1 = torch.zeros(1, requires_grad=True)\n",
    "w2 = torch.zeros(1, requires_grad=True)\n",
    "w3 = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 10000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, w1.item(), w3.item(), w3.item(), b.item(), cost.item()\n",
    "        ))\n",
    "\n",
    "def f(x1,x2,x3):\n",
    "    return x1 * w1 + x2 * w2 + x3 * w3 + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([151.3581], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(73,80,75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Data Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{pmatrix}\n",
    "x_1 & x_2 & x_3\n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "\\begin{pmatrix}\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "w_3 \\\\\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "x_1w_1 + x_2w_2 + x_3w_3\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ H(X) = XW $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/200 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch    1/200 hypothesis: tensor([67.2578, 80.8397, 79.6523, 86.7394, 61.6605]) Cost: 9298.520508\n",
      "Epoch    2/200 hypothesis: tensor([104.9128, 126.0990, 124.2466, 135.3015,  96.1821]) Cost: 2915.712891\n",
      "Epoch    3/200 hypothesis: tensor([125.9942, 151.4381, 149.2133, 162.4896, 115.5097]) Cost: 915.040527\n",
      "Epoch    4/200 hypothesis: tensor([137.7968, 165.6247, 163.1911, 177.7112, 126.3307]) Cost: 287.936005\n",
      "Epoch    5/200 hypothesis: tensor([144.4044, 173.5674, 171.0168, 186.2332, 132.3891]) Cost: 91.371010\n",
      "Epoch    6/200 hypothesis: tensor([148.1035, 178.0144, 175.3980, 191.0042, 135.7812]) Cost: 29.758139\n",
      "Epoch    7/200 hypothesis: tensor([150.1744, 180.5042, 177.8508, 193.6753, 137.6805]) Cost: 10.445305\n",
      "Epoch    8/200 hypothesis: tensor([151.3336, 181.8983, 179.2240, 195.1707, 138.7440]) Cost: 4.391228\n",
      "Epoch    9/200 hypothesis: tensor([151.9824, 182.6789, 179.9928, 196.0079, 139.3396]) Cost: 2.493135\n",
      "Epoch   10/200 hypothesis: tensor([152.3454, 183.1161, 180.4231, 196.4765, 139.6732]) Cost: 1.897688\n",
      "Epoch   11/200 hypothesis: tensor([152.5485, 183.3610, 180.6640, 196.7389, 139.8602]) Cost: 1.710541\n",
      "Epoch   12/200 hypothesis: tensor([152.6620, 183.4982, 180.7988, 196.8857, 139.9651]) Cost: 1.651412\n",
      "Epoch   13/200 hypothesis: tensor([152.7253, 183.5752, 180.8742, 196.9678, 140.0240]) Cost: 1.632387\n",
      "Epoch   14/200 hypothesis: tensor([152.7606, 183.6184, 180.9164, 197.0138, 140.0571]) Cost: 1.625923\n",
      "Epoch   15/200 hypothesis: tensor([152.7802, 183.6427, 180.9399, 197.0395, 140.0759]) Cost: 1.623412\n",
      "Epoch   16/200 hypothesis: tensor([152.7909, 183.6565, 180.9530, 197.0538, 140.0865]) Cost: 1.622141\n",
      "Epoch   17/200 hypothesis: tensor([152.7968, 183.6643, 180.9603, 197.0618, 140.0927]) Cost: 1.621253\n",
      "Epoch   18/200 hypothesis: tensor([152.7999, 183.6688, 180.9644, 197.0662, 140.0963]) Cost: 1.620500\n",
      "Epoch   19/200 hypothesis: tensor([152.8014, 183.6715, 180.9666, 197.0686, 140.0985]) Cost: 1.619770\n",
      "Epoch   20/200 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.1000]) Cost: 1.619033\n",
      "Epoch   21/200 hypothesis: tensor([152.8022, 183.6741, 180.9683, 197.0706, 140.1009]) Cost: 1.618346\n",
      "Epoch   22/200 hypothesis: tensor([152.8021, 183.6749, 180.9686, 197.0709, 140.1016]) Cost: 1.617637\n",
      "Epoch   23/200 hypothesis: tensor([152.8019, 183.6754, 180.9687, 197.0710, 140.1022]) Cost: 1.616930\n",
      "Epoch   24/200 hypothesis: tensor([152.8016, 183.6758, 180.9687, 197.0711, 140.1027]) Cost: 1.616221\n",
      "Epoch   25/200 hypothesis: tensor([152.8012, 183.6762, 180.9686, 197.0710, 140.1032]) Cost: 1.615508\n",
      "Epoch   26/200 hypothesis: tensor([152.8008, 183.6765, 180.9686, 197.0710, 140.1036]) Cost: 1.614815\n",
      "Epoch   27/200 hypothesis: tensor([152.8004, 183.6768, 180.9684, 197.0709, 140.1041]) Cost: 1.614109\n",
      "Epoch   28/200 hypothesis: tensor([152.8000, 183.6772, 180.9683, 197.0707, 140.1045]) Cost: 1.613387\n",
      "Epoch   29/200 hypothesis: tensor([152.7995, 183.6775, 180.9682, 197.0706, 140.1049]) Cost: 1.612701\n",
      "Epoch   30/200 hypothesis: tensor([152.7991, 183.6778, 180.9681, 197.0705, 140.1053]) Cost: 1.611987\n",
      "Epoch   31/200 hypothesis: tensor([152.7987, 183.6781, 180.9679, 197.0704, 140.1057]) Cost: 1.611289\n",
      "Epoch   32/200 hypothesis: tensor([152.7982, 183.6784, 180.9678, 197.0703, 140.1061]) Cost: 1.610588\n",
      "Epoch   33/200 hypothesis: tensor([152.7978, 183.6787, 180.9677, 197.0702, 140.1065]) Cost: 1.609874\n",
      "Epoch   34/200 hypothesis: tensor([152.7974, 183.6790, 180.9676, 197.0701, 140.1069]) Cost: 1.609183\n",
      "Epoch   35/200 hypothesis: tensor([152.7969, 183.6793, 180.9674, 197.0700, 140.1073]) Cost: 1.608466\n",
      "Epoch   36/200 hypothesis: tensor([152.7965, 183.6796, 180.9673, 197.0699, 140.1078]) Cost: 1.607776\n",
      "Epoch   37/200 hypothesis: tensor([152.7961, 183.6799, 180.9672, 197.0698, 140.1082]) Cost: 1.607080\n",
      "Epoch   38/200 hypothesis: tensor([152.7957, 183.6802, 180.9670, 197.0697, 140.1086]) Cost: 1.606384\n",
      "Epoch   39/200 hypothesis: tensor([152.7952, 183.6805, 180.9669, 197.0695, 140.1090]) Cost: 1.605665\n",
      "Epoch   40/200 hypothesis: tensor([152.7948, 183.6807, 180.9668, 197.0694, 140.1094]) Cost: 1.604974\n",
      "Epoch   41/200 hypothesis: tensor([152.7944, 183.6810, 180.9666, 197.0693, 140.1098]) Cost: 1.604266\n",
      "Epoch   42/200 hypothesis: tensor([152.7939, 183.6813, 180.9665, 197.0692, 140.1102]) Cost: 1.603586\n",
      "Epoch   43/200 hypothesis: tensor([152.7935, 183.6816, 180.9664, 197.0691, 140.1106]) Cost: 1.602878\n",
      "Epoch   44/200 hypothesis: tensor([152.7931, 183.6819, 180.9663, 197.0690, 140.1110]) Cost: 1.602179\n",
      "Epoch   45/200 hypothesis: tensor([152.7926, 183.6822, 180.9661, 197.0689, 140.1114]) Cost: 1.601486\n",
      "Epoch   46/200 hypothesis: tensor([152.7922, 183.6826, 180.9660, 197.0688, 140.1118]) Cost: 1.600765\n",
      "Epoch   47/200 hypothesis: tensor([152.7918, 183.6828, 180.9659, 197.0686, 140.1122]) Cost: 1.600072\n",
      "Epoch   48/200 hypothesis: tensor([152.7914, 183.6831, 180.9657, 197.0685, 140.1126]) Cost: 1.599369\n",
      "Epoch   49/200 hypothesis: tensor([152.7909, 183.6834, 180.9656, 197.0684, 140.1130]) Cost: 1.598692\n",
      "Epoch   50/200 hypothesis: tensor([152.7905, 183.6837, 180.9655, 197.0683, 140.1134]) Cost: 1.597991\n",
      "Epoch   51/200 hypothesis: tensor([152.7900, 183.6840, 180.9653, 197.0682, 140.1138]) Cost: 1.597289\n",
      "Epoch   52/200 hypothesis: tensor([152.7896, 183.6843, 180.9652, 197.0681, 140.1143]) Cost: 1.596590\n",
      "Epoch   53/200 hypothesis: tensor([152.7892, 183.6846, 180.9651, 197.0679, 140.1147]) Cost: 1.595898\n",
      "Epoch   54/200 hypothesis: tensor([152.7888, 183.6849, 180.9650, 197.0678, 140.1151]) Cost: 1.595198\n",
      "Epoch   55/200 hypothesis: tensor([152.7883, 183.6852, 180.9648, 197.0677, 140.1155]) Cost: 1.594514\n",
      "Epoch   56/200 hypothesis: tensor([152.7879, 183.6855, 180.9647, 197.0676, 140.1159]) Cost: 1.593821\n",
      "Epoch   57/200 hypothesis: tensor([152.7875, 183.6858, 180.9646, 197.0675, 140.1163]) Cost: 1.593129\n",
      "Epoch   58/200 hypothesis: tensor([152.7870, 183.6861, 180.9644, 197.0674, 140.1167]) Cost: 1.592433\n",
      "Epoch   59/200 hypothesis: tensor([152.7866, 183.6864, 180.9643, 197.0673, 140.1171]) Cost: 1.591737\n",
      "Epoch   60/200 hypothesis: tensor([152.7862, 183.6867, 180.9642, 197.0672, 140.1175]) Cost: 1.591038\n",
      "Epoch   61/200 hypothesis: tensor([152.7858, 183.6870, 180.9641, 197.0670, 140.1179]) Cost: 1.590336\n",
      "Epoch   62/200 hypothesis: tensor([152.7853, 183.6873, 180.9639, 197.0669, 140.1183]) Cost: 1.589665\n",
      "Epoch   63/200 hypothesis: tensor([152.7849, 183.6876, 180.9638, 197.0668, 140.1187]) Cost: 1.588963\n",
      "Epoch   64/200 hypothesis: tensor([152.7845, 183.6879, 180.9637, 197.0667, 140.1191]) Cost: 1.588273\n",
      "Epoch   65/200 hypothesis: tensor([152.7840, 183.6882, 180.9635, 197.0666, 140.1195]) Cost: 1.587576\n",
      "Epoch   66/200 hypothesis: tensor([152.7836, 183.6885, 180.9634, 197.0665, 140.1199]) Cost: 1.586890\n",
      "Epoch   67/200 hypothesis: tensor([152.7832, 183.6888, 180.9633, 197.0664, 140.1203]) Cost: 1.586196\n",
      "Epoch   68/200 hypothesis: tensor([152.7828, 183.6890, 180.9632, 197.0663, 140.1207]) Cost: 1.585522\n",
      "Epoch   69/200 hypothesis: tensor([152.7823, 183.6893, 180.9630, 197.0661, 140.1211]) Cost: 1.584821\n",
      "Epoch   70/200 hypothesis: tensor([152.7819, 183.6896, 180.9629, 197.0660, 140.1215]) Cost: 1.584131\n",
      "Epoch   71/200 hypothesis: tensor([152.7815, 183.6899, 180.9628, 197.0659, 140.1219]) Cost: 1.583444\n",
      "Epoch   72/200 hypothesis: tensor([152.7810, 183.6902, 180.9626, 197.0658, 140.1223]) Cost: 1.582750\n",
      "Epoch   73/200 hypothesis: tensor([152.7806, 183.6905, 180.9625, 197.0657, 140.1227]) Cost: 1.582072\n",
      "Epoch   74/200 hypothesis: tensor([152.7802, 183.6908, 180.9624, 197.0656, 140.1231]) Cost: 1.581385\n",
      "Epoch   75/200 hypothesis: tensor([152.7798, 183.6911, 180.9622, 197.0655, 140.1236]) Cost: 1.580690\n",
      "Epoch   76/200 hypothesis: tensor([152.7793, 183.6914, 180.9621, 197.0654, 140.1240]) Cost: 1.579996\n",
      "Epoch   77/200 hypothesis: tensor([152.7789, 183.6917, 180.9620, 197.0653, 140.1244]) Cost: 1.579299\n",
      "Epoch   78/200 hypothesis: tensor([152.7785, 183.6920, 180.9619, 197.0651, 140.1248]) Cost: 1.578632\n",
      "Epoch   79/200 hypothesis: tensor([152.7781, 183.6923, 180.9617, 197.0650, 140.1252]) Cost: 1.577947\n",
      "Epoch   80/200 hypothesis: tensor([152.7776, 183.6926, 180.9616, 197.0649, 140.1256]) Cost: 1.577271\n",
      "Epoch   81/200 hypothesis: tensor([152.7772, 183.6929, 180.9615, 197.0648, 140.1260]) Cost: 1.576558\n",
      "Epoch   82/200 hypothesis: tensor([152.7768, 183.6932, 180.9614, 197.0647, 140.1264]) Cost: 1.575896\n",
      "Epoch   83/200 hypothesis: tensor([152.7764, 183.6935, 180.9612, 197.0646, 140.1268]) Cost: 1.575205\n",
      "Epoch   84/200 hypothesis: tensor([152.7759, 183.6938, 180.9611, 197.0645, 140.1272]) Cost: 1.574519\n",
      "Epoch   85/200 hypothesis: tensor([152.7755, 183.6940, 180.9610, 197.0643, 140.1276]) Cost: 1.573839\n",
      "Epoch   86/200 hypothesis: tensor([152.7751, 183.6944, 180.9609, 197.0643, 140.1280]) Cost: 1.573149\n",
      "Epoch   87/200 hypothesis: tensor([152.7747, 183.6947, 180.9607, 197.0641, 140.1284]) Cost: 1.572457\n",
      "Epoch   88/200 hypothesis: tensor([152.7742, 183.6949, 180.9606, 197.0640, 140.1288]) Cost: 1.571791\n",
      "Epoch   89/200 hypothesis: tensor([152.7738, 183.6952, 180.9605, 197.0639, 140.1292]) Cost: 1.571114\n",
      "Epoch   90/200 hypothesis: tensor([152.7734, 183.6955, 180.9603, 197.0638, 140.1296]) Cost: 1.570435\n",
      "Epoch   91/200 hypothesis: tensor([152.7729, 183.6958, 180.9602, 197.0637, 140.1300]) Cost: 1.569754\n",
      "Epoch   92/200 hypothesis: tensor([152.7725, 183.6961, 180.9601, 197.0636, 140.1304]) Cost: 1.569064\n",
      "Epoch   93/200 hypothesis: tensor([152.7721, 183.6964, 180.9600, 197.0634, 140.1308]) Cost: 1.568386\n",
      "Epoch   94/200 hypothesis: tensor([152.7717, 183.6967, 180.9598, 197.0633, 140.1312]) Cost: 1.567708\n",
      "Epoch   95/200 hypothesis: tensor([152.7713, 183.6970, 180.9597, 197.0632, 140.1316]) Cost: 1.567021\n",
      "Epoch   96/200 hypothesis: tensor([152.7708, 183.6973, 180.9596, 197.0631, 140.1320]) Cost: 1.566340\n",
      "Epoch   97/200 hypothesis: tensor([152.7704, 183.6976, 180.9594, 197.0630, 140.1324]) Cost: 1.565658\n",
      "Epoch   98/200 hypothesis: tensor([152.7700, 183.6979, 180.9593, 197.0629, 140.1328]) Cost: 1.564987\n",
      "Epoch   99/200 hypothesis: tensor([152.7695, 183.6982, 180.9592, 197.0628, 140.1332]) Cost: 1.564298\n",
      "Epoch  100/200 hypothesis: tensor([152.7691, 183.6985, 180.9591, 197.0627, 140.1336]) Cost: 1.563634\n",
      "Epoch  101/200 hypothesis: tensor([152.7687, 183.6987, 180.9589, 197.0626, 140.1340]) Cost: 1.562956\n",
      "Epoch  102/200 hypothesis: tensor([152.7683, 183.6990, 180.9588, 197.0625, 140.1344]) Cost: 1.562279\n",
      "Epoch  103/200 hypothesis: tensor([152.7679, 183.6993, 180.9587, 197.0624, 140.1348]) Cost: 1.561604\n",
      "Epoch  104/200 hypothesis: tensor([152.7674, 183.6996, 180.9585, 197.0622, 140.1352]) Cost: 1.560926\n",
      "Epoch  105/200 hypothesis: tensor([152.7670, 183.6999, 180.9584, 197.0621, 140.1356]) Cost: 1.560241\n",
      "Epoch  106/200 hypothesis: tensor([152.7666, 183.7002, 180.9583, 197.0620, 140.1360]) Cost: 1.559565\n",
      "Epoch  107/200 hypothesis: tensor([152.7662, 183.7005, 180.9582, 197.0619, 140.1364]) Cost: 1.558889\n",
      "Epoch  108/200 hypothesis: tensor([152.7657, 183.7008, 180.9580, 197.0618, 140.1368]) Cost: 1.558209\n",
      "Epoch  109/200 hypothesis: tensor([152.7653, 183.7011, 180.9579, 197.0617, 140.1372]) Cost: 1.557538\n",
      "Epoch  110/200 hypothesis: tensor([152.7649, 183.7014, 180.9578, 197.0616, 140.1376]) Cost: 1.556869\n",
      "Epoch  111/200 hypothesis: tensor([152.7645, 183.7017, 180.9577, 197.0615, 140.1380]) Cost: 1.556181\n",
      "Epoch  112/200 hypothesis: tensor([152.7640, 183.7020, 180.9575, 197.0613, 140.1384]) Cost: 1.555500\n",
      "Epoch  113/200 hypothesis: tensor([152.7636, 183.7023, 180.9574, 197.0612, 140.1388]) Cost: 1.554848\n",
      "Epoch  114/200 hypothesis: tensor([152.7632, 183.7025, 180.9573, 197.0611, 140.1392]) Cost: 1.554168\n",
      "Epoch  115/200 hypothesis: tensor([152.7628, 183.7029, 180.9572, 197.0610, 140.1396]) Cost: 1.553490\n",
      "Epoch  116/200 hypothesis: tensor([152.7624, 183.7031, 180.9570, 197.0609, 140.1400]) Cost: 1.552834\n",
      "Epoch  117/200 hypothesis: tensor([152.7619, 183.7034, 180.9569, 197.0608, 140.1404]) Cost: 1.552154\n",
      "Epoch  118/200 hypothesis: tensor([152.7615, 183.7037, 180.9568, 197.0607, 140.1408]) Cost: 1.551474\n",
      "Epoch  119/200 hypothesis: tensor([152.7611, 183.7040, 180.9566, 197.0606, 140.1412]) Cost: 1.550800\n",
      "Epoch  120/200 hypothesis: tensor([152.7607, 183.7043, 180.9565, 197.0604, 140.1416]) Cost: 1.550140\n",
      "Epoch  121/200 hypothesis: tensor([152.7603, 183.7046, 180.9564, 197.0603, 140.1420]) Cost: 1.549464\n",
      "Epoch  122/200 hypothesis: tensor([152.7598, 183.7049, 180.9563, 197.0602, 140.1424]) Cost: 1.548791\n",
      "Epoch  123/200 hypothesis: tensor([152.7594, 183.7052, 180.9561, 197.0601, 140.1428]) Cost: 1.548118\n",
      "Epoch  124/200 hypothesis: tensor([152.7590, 183.7054, 180.9560, 197.0600, 140.1432]) Cost: 1.547476\n",
      "Epoch  125/200 hypothesis: tensor([152.7586, 183.7057, 180.9559, 197.0599, 140.1436]) Cost: 1.546786\n",
      "Epoch  126/200 hypothesis: tensor([152.7581, 183.7061, 180.9558, 197.0598, 140.1440]) Cost: 1.546116\n",
      "Epoch  127/200 hypothesis: tensor([152.7577, 183.7063, 180.9556, 197.0597, 140.1444]) Cost: 1.545445\n",
      "Epoch  128/200 hypothesis: tensor([152.7573, 183.7066, 180.9555, 197.0596, 140.1448]) Cost: 1.544772\n",
      "Epoch  129/200 hypothesis: tensor([152.7569, 183.7069, 180.9554, 197.0595, 140.1452]) Cost: 1.544119\n",
      "Epoch  130/200 hypothesis: tensor([152.7565, 183.7072, 180.9552, 197.0594, 140.1456]) Cost: 1.543452\n",
      "Epoch  131/200 hypothesis: tensor([152.7560, 183.7075, 180.9551, 197.0592, 140.1460]) Cost: 1.542774\n",
      "Epoch  132/200 hypothesis: tensor([152.7556, 183.7078, 180.9550, 197.0591, 140.1463]) Cost: 1.542113\n",
      "Epoch  133/200 hypothesis: tensor([152.7552, 183.7081, 180.9549, 197.0590, 140.1467]) Cost: 1.541449\n",
      "Epoch  134/200 hypothesis: tensor([152.7548, 183.7084, 180.9547, 197.0589, 140.1472]) Cost: 1.540778\n",
      "Epoch  135/200 hypothesis: tensor([152.7544, 183.7086, 180.9546, 197.0588, 140.1476]) Cost: 1.540118\n",
      "Epoch  136/200 hypothesis: tensor([152.7539, 183.7089, 180.9545, 197.0587, 140.1479]) Cost: 1.539458\n",
      "Epoch  137/200 hypothesis: tensor([152.7535, 183.7092, 180.9544, 197.0586, 140.1483]) Cost: 1.538791\n",
      "Epoch  138/200 hypothesis: tensor([152.7531, 183.7095, 180.9543, 197.0585, 140.1487]) Cost: 1.538115\n",
      "Epoch  139/200 hypothesis: tensor([152.7527, 183.7098, 180.9541, 197.0584, 140.1491]) Cost: 1.537469\n",
      "Epoch  140/200 hypothesis: tensor([152.7523, 183.7101, 180.9540, 197.0582, 140.1495]) Cost: 1.536793\n",
      "Epoch  141/200 hypothesis: tensor([152.7518, 183.7104, 180.9539, 197.0581, 140.1499]) Cost: 1.536123\n",
      "Epoch  142/200 hypothesis: tensor([152.7514, 183.7107, 180.9537, 197.0580, 140.1503]) Cost: 1.535471\n",
      "Epoch  143/200 hypothesis: tensor([152.7510, 183.7110, 180.9536, 197.0579, 140.1507]) Cost: 1.534806\n",
      "Epoch  144/200 hypothesis: tensor([152.7506, 183.7113, 180.9535, 197.0578, 140.1511]) Cost: 1.534137\n",
      "Epoch  145/200 hypothesis: tensor([152.7502, 183.7115, 180.9534, 197.0577, 140.1515]) Cost: 1.533475\n",
      "Epoch  146/200 hypothesis: tensor([152.7497, 183.7118, 180.9532, 197.0576, 140.1519]) Cost: 1.532804\n",
      "Epoch  147/200 hypothesis: tensor([152.7493, 183.7121, 180.9531, 197.0575, 140.1523]) Cost: 1.532158\n",
      "Epoch  148/200 hypothesis: tensor([152.7489, 183.7124, 180.9530, 197.0574, 140.1527]) Cost: 1.531501\n",
      "Epoch  149/200 hypothesis: tensor([152.7485, 183.7127, 180.9529, 197.0573, 140.1531]) Cost: 1.530844\n",
      "Epoch  150/200 hypothesis: tensor([152.7481, 183.7130, 180.9527, 197.0571, 140.1535]) Cost: 1.530177\n",
      "Epoch  151/200 hypothesis: tensor([152.7477, 183.7133, 180.9526, 197.0570, 140.1539]) Cost: 1.529519\n",
      "Epoch  152/200 hypothesis: tensor([152.7472, 183.7136, 180.9525, 197.0569, 140.1543]) Cost: 1.528859\n",
      "Epoch  153/200 hypothesis: tensor([152.7468, 183.7139, 180.9523, 197.0568, 140.1547]) Cost: 1.528190\n",
      "Epoch  154/200 hypothesis: tensor([152.7464, 183.7141, 180.9522, 197.0567, 140.1551]) Cost: 1.527539\n",
      "Epoch  155/200 hypothesis: tensor([152.7460, 183.7144, 180.9521, 197.0566, 140.1555]) Cost: 1.526873\n",
      "Epoch  156/200 hypothesis: tensor([152.7456, 183.7147, 180.9520, 197.0565, 140.1559]) Cost: 1.526216\n",
      "Epoch  157/200 hypothesis: tensor([152.7452, 183.7150, 180.9519, 197.0564, 140.1563]) Cost: 1.525557\n",
      "Epoch  158/200 hypothesis: tensor([152.7447, 183.7153, 180.9517, 197.0563, 140.1566]) Cost: 1.524909\n",
      "Epoch  159/200 hypothesis: tensor([152.7443, 183.7156, 180.9516, 197.0562, 140.1570]) Cost: 1.524255\n",
      "Epoch  160/200 hypothesis: tensor([152.7439, 183.7159, 180.9515, 197.0560, 140.1574]) Cost: 1.523593\n",
      "Epoch  161/200 hypothesis: tensor([152.7435, 183.7162, 180.9513, 197.0559, 140.1578]) Cost: 1.522932\n",
      "Epoch  162/200 hypothesis: tensor([152.7431, 183.7164, 180.9512, 197.0558, 140.1582]) Cost: 1.522274\n",
      "Epoch  163/200 hypothesis: tensor([152.7426, 183.7167, 180.9511, 197.0557, 140.1586]) Cost: 1.521627\n",
      "Epoch  164/200 hypothesis: tensor([152.7422, 183.7170, 180.9510, 197.0556, 140.1590]) Cost: 1.520965\n",
      "Epoch  165/200 hypothesis: tensor([152.7418, 183.7173, 180.9509, 197.0555, 140.1594]) Cost: 1.520317\n",
      "Epoch  166/200 hypothesis: tensor([152.7414, 183.7176, 180.9507, 197.0554, 140.1598]) Cost: 1.519655\n",
      "Epoch  167/200 hypothesis: tensor([152.7410, 183.7179, 180.9506, 197.0553, 140.1602]) Cost: 1.519011\n",
      "Epoch  168/200 hypothesis: tensor([152.7406, 183.7182, 180.9505, 197.0552, 140.1606]) Cost: 1.518353\n",
      "Epoch  169/200 hypothesis: tensor([152.7401, 183.7185, 180.9503, 197.0551, 140.1610]) Cost: 1.517682\n",
      "Epoch  170/200 hypothesis: tensor([152.7397, 183.7188, 180.9502, 197.0549, 140.1614]) Cost: 1.517036\n",
      "Epoch  171/200 hypothesis: tensor([152.7393, 183.7190, 180.9501, 197.0548, 140.1618]) Cost: 1.516394\n",
      "Epoch  172/200 hypothesis: tensor([152.7389, 183.7193, 180.9500, 197.0547, 140.1622]) Cost: 1.515729\n",
      "Epoch  173/200 hypothesis: tensor([152.7385, 183.7196, 180.9499, 197.0546, 140.1625]) Cost: 1.515094\n",
      "Epoch  174/200 hypothesis: tensor([152.7381, 183.7199, 180.9497, 197.0545, 140.1629]) Cost: 1.514436\n",
      "Epoch  175/200 hypothesis: tensor([152.7376, 183.7202, 180.9496, 197.0544, 140.1633]) Cost: 1.513790\n",
      "Epoch  176/200 hypothesis: tensor([152.7372, 183.7205, 180.9495, 197.0543, 140.1637]) Cost: 1.513120\n",
      "Epoch  177/200 hypothesis: tensor([152.7368, 183.7208, 180.9494, 197.0542, 140.1641]) Cost: 1.512470\n",
      "Epoch  178/200 hypothesis: tensor([152.7364, 183.7211, 180.9492, 197.0541, 140.1645]) Cost: 1.511832\n",
      "Epoch  179/200 hypothesis: tensor([152.7360, 183.7213, 180.9491, 197.0539, 140.1649]) Cost: 1.511169\n",
      "Epoch  180/200 hypothesis: tensor([152.7356, 183.7216, 180.9490, 197.0538, 140.1653]) Cost: 1.510536\n",
      "Epoch  181/200 hypothesis: tensor([152.7352, 183.7219, 180.9488, 197.0537, 140.1657]) Cost: 1.509874\n",
      "Epoch  182/200 hypothesis: tensor([152.7347, 183.7222, 180.9487, 197.0536, 140.1661]) Cost: 1.509222\n",
      "Epoch  183/200 hypothesis: tensor([152.7343, 183.7225, 180.9486, 197.0535, 140.1665]) Cost: 1.508582\n",
      "Epoch  184/200 hypothesis: tensor([152.7339, 183.7228, 180.9485, 197.0534, 140.1669]) Cost: 1.507932\n",
      "Epoch  185/200 hypothesis: tensor([152.7335, 183.7230, 180.9483, 197.0533, 140.1673]) Cost: 1.507282\n",
      "Epoch  186/200 hypothesis: tensor([152.7331, 183.7233, 180.9482, 197.0532, 140.1676]) Cost: 1.506649\n",
      "Epoch  187/200 hypothesis: tensor([152.7327, 183.7236, 180.9481, 197.0531, 140.1680]) Cost: 1.505989\n",
      "Epoch  188/200 hypothesis: tensor([152.7323, 183.7239, 180.9480, 197.0530, 140.1684]) Cost: 1.505345\n",
      "Epoch  189/200 hypothesis: tensor([152.7318, 183.7242, 180.9479, 197.0529, 140.1688]) Cost: 1.504684\n",
      "Epoch  190/200 hypothesis: tensor([152.7314, 183.7245, 180.9477, 197.0527, 140.1692]) Cost: 1.504053\n",
      "Epoch  191/200 hypothesis: tensor([152.7310, 183.7248, 180.9476, 197.0526, 140.1696]) Cost: 1.503414\n",
      "Epoch  192/200 hypothesis: tensor([152.7306, 183.7251, 180.9475, 197.0525, 140.1700]) Cost: 1.502758\n",
      "Epoch  193/200 hypothesis: tensor([152.7302, 183.7253, 180.9474, 197.0524, 140.1704]) Cost: 1.502116\n",
      "Epoch  194/200 hypothesis: tensor([152.7298, 183.7256, 180.9472, 197.0523, 140.1708]) Cost: 1.501467\n",
      "Epoch  195/200 hypothesis: tensor([152.7294, 183.7259, 180.9471, 197.0522, 140.1712]) Cost: 1.500828\n",
      "Epoch  196/200 hypothesis: tensor([152.7289, 183.7262, 180.9470, 197.0521, 140.1715]) Cost: 1.500179\n",
      "Epoch  197/200 hypothesis: tensor([152.7285, 183.7265, 180.9469, 197.0520, 140.1719]) Cost: 1.499527\n",
      "Epoch  198/200 hypothesis: tensor([152.7281, 183.7268, 180.9467, 197.0519, 140.1723]) Cost: 1.498881\n",
      "Epoch  199/200 hypothesis: tensor([152.7277, 183.7271, 180.9466, 197.0518, 140.1727]) Cost: 1.498252\n",
      "Epoch  200/200 hypothesis: tensor([152.7273, 183.7273, 180.9465, 197.0517, 140.1731]) Cost: 1.497603\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "W = torch.zeros((3, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 200\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    hypothesis = x_train.matmul(W) + b # or .mm or @\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level Implementation with `nn.Module`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you remember this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just need to change the input dimension from 1 to 3!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Cost: 31667.599609\n",
      "Epoch    1/20 Cost: 9926.265625\n",
      "Epoch    2/20 Cost: 3111.513916\n",
      "Epoch    3/20 Cost: 975.451355\n",
      "Epoch    4/20 Cost: 305.908539\n",
      "Epoch    5/20 Cost: 96.042496\n",
      "Epoch    6/20 Cost: 30.260748\n",
      "Epoch    7/20 Cost: 9.641701\n",
      "Epoch    8/20 Cost: 3.178671\n",
      "Epoch    9/20 Cost: 1.152871\n",
      "Epoch   10/20 Cost: 0.517863\n",
      "Epoch   11/20 Cost: 0.318801\n",
      "Epoch   12/20 Cost: 0.256388\n",
      "Epoch   13/20 Cost: 0.236821\n",
      "Epoch   14/20 Cost: 0.230660\n",
      "Epoch   15/20 Cost: 0.228719\n",
      "Epoch   16/20 Cost: 0.228095\n",
      "Epoch   17/20 Cost: 0.227880\n",
      "Epoch   18/20 Cost: 0.227799\n",
      "Epoch   19/20 Cost: 0.227762\n",
      "Epoch   20/20 Cost: 0.227732\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "# 모델 초기화\n",
    "model = MultivariateLinearRegressionModel()\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs+1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 20번마다 로그 출력\n",
    "    print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, cost.item()\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
